{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from time import time\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors                         \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vocabulary: make a list of all words seen in the training set. Lowercase all words, include punctuation marks as well. Save the vocabulary to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`’:]\", \" \", string)  \n",
    "    string = re.sub(r\"’\", \"'\", string) \n",
    "    string = re.sub(r\"`\", \"'\", string) \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\":\", \" : \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" ( \", string) \n",
    "    string = re.sub(r\"\\)\", \" ) \", string) \n",
    "    string = re.sub(r\"\\?\", \" ? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip()\n",
    "\n",
    "def update_vocabulary(tokens, counter):\n",
    "    counter.update(tokens)   \n",
    "\n",
    "def create_vocab(pd_series, min_count_word=3):\n",
    "    vocabulary = Counter()\n",
    "    _ = pd_series.apply(lambda x: update_vocabulary(tokenize_string(x).casefold().split(), vocabulary))\n",
    "    vocabulary = [tok for tok, count in vocabulary.items() if count >= min_count_word]\n",
    "    vocabulary += ['<PAD>']\n",
    "    return vocabulary\n",
    "\n",
    "def save_vocab_to_txt_file(vocab, txt_path):\n",
    "    \"\"\"Writes one token per line, 0-based line id corresponds to the id of the token.\n",
    "    Args:\n",
    "        vocab: (iterable object) yields token\n",
    "        txt_path: (stirng) path to vocab file\n",
    "    \"\"\"\n",
    "    with open(txt_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(token for token in vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data, tokenise sentence, remove examples with empty questions. Split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv',\n",
    "                   names=[\"id\",\"qid1\",\"qid2\",\"question1\",\"question2\",\"is_duplicate\"],\n",
    "                   skiprows=1).fillna({'question1': ' ', 'question2': ' '})\n",
    "data['question1'] = data['question1'].apply(lambda string: tokenize_string(string).casefold())\n",
    "data['question2'] = data['question2'].apply(lambda string: tokenize_string(string).casefold())\n",
    "data.drop(index=data[(data['question2'].apply(lambda s: len(s.split()))==0) | \n",
    "                     (data['question1'].apply(lambda s: len(s.split()))==0)].index, inplace=True)\n",
    "data = data.reindex(index=np.random.permutation(data.index))\n",
    "data['is_duplicate'] = data['is_duplicate'].astype(np.float32)\n",
    "data, data_te = train_test_split(data, test_size=.1, stratify=data['is_duplicate'], random_state=17)\n",
    "\n",
    "# Make vocabulary from training data\n",
    "vocabulary = create_vocab(pd.concat((data['question1'], data['question2'])))\n",
    "vocabulary_size = len(vocabulary)+1\n",
    "save_vocab_to_txt_file(vocabulary, 'data/words.txt')\n",
    "\n",
    "# Load and preprocess submission data\n",
    "data_sub = pd.read_csv('data/test.csv').fillna({'question1': ' ', 'question2': ' '}) \n",
    "data_sub['test_id']=data_sub['test_id'].apply(pd.to_numeric, errors='coerce')\n",
    "data_sub.dropna(subset=['test_id'], inplace=True)\n",
    "data_sub['test_id'] = data_sub['test_id'].astype(int)\n",
    "data_sub = data_sub.drop_duplicates(subset=['test_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GoogleNews-vectors-negative300.bin pre-trained word2vec model\n",
    "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only word2vec embeddings for those words which are present in the training set \n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./w2v/GoogleNews-vectors-negative300.bin', binary=True) \n",
    "emb_size = 22 \n",
    "embeddings = np.random.uniform(-.3, .3, (vocabulary_size+1, 300)).astype(np.float32)\n",
    "\n",
    "for i, item in enumerate(vocabulary):\n",
    "    if item in model:\n",
    "        embeddings[i] = model[item]\n",
    "# Only about 35 000 tokens from word2vec are used, 300 dimensions may be superfluous therefore\n",
    "pca = PCA(n_components=emb_size)\n",
    "embeddings = pca.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pipeline. The data is consumed from pandas DataFrame. The sentences are converted into series of word indicies which will further be used to look up word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(file_name):\n",
    "    words = tf.contrib.lookup.index_table_from_file(file_name, num_oov_buckets=1, delimiter='\\n', name='vocab', )\n",
    "    tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, words.init)\n",
    "    return words\n",
    "\n",
    "def preprocess_sentence(sentence, vocab):\n",
    "    return tf.data.Dataset.from_tensor_slices(sentence)  \\\n",
    "                .map(lambda string: tf.string_split([string]).values)  \\\n",
    "                .map(lambda tokens: (vocab.lookup(tokens)))\n",
    "\n",
    "def load_dataset_from_pd(pd_dframe, vocab):\n",
    "    \"\"\"Create tf.data Instance from txt file\n",
    "    Args:\n",
    "        path_txt: (string) path containing one example per line\n",
    "        vocab: (tf.lookuptable)\n",
    "    Returns:\n",
    "        dataset: (tf.Dataset) yielding list of ids of tokens for each example\n",
    "    \"\"\"\n",
    "    sentence1 = preprocess_sentence(pd_dframe['question1'].values, vocab)\n",
    "    sentence2 = preprocess_sentence(pd_dframe['question2'].values, vocab)\n",
    "    return tf.data.Dataset.zip((sentence1, sentence2))\n",
    "\n",
    "def input_fn(mode,\n",
    "             features,\n",
    "             labels,\n",
    "             params):\n",
    "    \n",
    "    is_training_or_eval = (mode == 'train' or mode == 'eval')\n",
    "    is_training = (mode == 'train')\n",
    "    buffer_size = params['buffer_size'] if is_training else 1\n",
    "    vocab = build_vocab(params['vocabulary_path'])\n",
    "    id_pad_word = vocab.lookup(tf.constant('<PAD>'))\n",
    "    features = load_dataset_from_pd(features, vocab)\n",
    "\n",
    "    # sentence of unknown size\n",
    "    padded_shapes = (\n",
    "        (tf.TensorShape([None]), tf.TensorShape([None])),\n",
    "                     tf.TensorShape([None])\n",
    "    )    \n",
    "    # sentence padded on the right with id_pad_word\n",
    "    padding_values = ((id_pad_word, id_pad_word),\n",
    "                      tf.constant(0, dtype=tf.float32))   \n",
    "    labels = tf.data.Dataset.from_tensor_slices(labels.values[:, np.newaxis])\n",
    "    dataset = tf.data.Dataset.zip((features, labels))\n",
    "\n",
    "    if params['shuffle']:\n",
    "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "    dataset = dataset.repeat(params['num_epochs']).padded_batch(params['batch_size'],\n",
    "                          padded_shapes=padded_shapes,\n",
    "                          padding_values=padding_values)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model: word index sequence -> embeddings sequence -> LSTM -> CNN layers -> rectification -> Max pooling -> softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_layer(net, n_kernels, k_size, stride):\n",
    "    net = tf.layers.conv1d(net, n_kernels, [k_size], stride) \n",
    "    net = tf.nn.relu(net)\n",
    "    net = tf.reduce_max(net, axis=-2)\n",
    "    net = tf.reshape(net, [-1, n_kernels])\n",
    "    return net\n",
    "\n",
    "def sentence_representation(sentences, params):\n",
    "    emb_const = tf.constant(params['embeddings'])\n",
    "    embeddings = tf.get_variable('embeddings',\n",
    "                                 initializer=emb_const\n",
    "                                )\n",
    "    net = tf.nn.embedding_lookup(embeddings, sentences)  \n",
    "#     net = tf.layers.dropout(net, .2)\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(params['n_units_lstm'], activation=tf.nn.relu)\n",
    "    lstm_o, lstm_state = tf.nn.dynamic_rnn(lstm_cell, net, dtype=net.dtype)\n",
    "    # Stack outputs from convolution layers with different kernel sizes into a vector\n",
    "    stack = tf.concat([convolution_layer(lstm_o, params['n_kernels'], size, stride=1)\n",
    "                       for size in params['filter_sizes']],\n",
    "                     axis=1)    \n",
    "    return stack\n",
    "\n",
    "def my_model(features, labels, mode, params):\n",
    "    # create two branches with same variables\n",
    "    sntc_repr = tf.make_template('sntc_repr', sentence_representation, params)\n",
    "    flat1 = sntc_repr(features[0], params)\n",
    "    flat2 = sntc_repr(features[1], params)\n",
    "    \n",
    "    net = tf.concat([flat1, flat2], 1)\n",
    "#     net = tf.layers.dense(net, params['dense_units'], activation=tf.nn.relu)\n",
    "#     net = tf.layers.dropout(net, 0.2)\n",
    "    logits = tf.layers.dense(net, 1)\n",
    "    probabilities = tf.nn.sigmoid(logits)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'probabilities': probabilities,\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "\n",
    "\n",
    "    metrics = {'auc': .5} #mockup\n",
    "    tf.summary.scalar('auc', .5) #mockup\n",
    "\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "    \n",
    "    # Create training op\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.002)\n",
    "    optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, 5.0)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params={'num_epochs': 30,\n",
    "             'shuffle': True,\n",
    "             'buffer_size': 300000,\n",
    "             'batch_size': 1000,\n",
    "             'vocabulary_path': 'data/words.txt'\n",
    "            }\n",
    "\n",
    "model_params = {'embedding_size': emb_size,\n",
    "                'embeddings': embeddings,\n",
    "                'n_units_lstm': 16,\n",
    "                'n_kernels': 32,\n",
    "                'vocabulary_size': vocabulary_size,\n",
    "                'filter_sizes': [2, 3, 4, 5],\n",
    "#                'dense_units': 80\n",
    "               }\n",
    "\n",
    "towers = tf.estimator.Estimator(model_fn=my_model,\n",
    "                           params=model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  70.96544480323792\n"
     ]
    }
   ],
   "source": [
    "s_time = time()\n",
    "_ = towers.train(input_fn=lambda: input_fn(tf.estimator.ModeKeys.TRAIN,\n",
    "                                      data,\n",
    "                                      data['is_duplicate'],\n",
    "                                      data_params),\n",
    "                 steps=1\n",
    "            )\n",
    "print('Training time: ', time()-s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params_pred={'num_epochs': 1,\n",
    "             'shuffle': False,\n",
    "             'buffer_size': 1,\n",
    "             'batch_size': 500,\n",
    "             'vocabulary_path': 'data/words.txt'\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction completed\n"
     ]
    }
   ],
   "source": [
    "s_time = time()\n",
    "prd = towers.predict(input_fn=lambda: input_fn(tf.estimator.ModeKeys.PREDICT,\n",
    "                                      data_te,\n",
    "                                      pd.Series(np.zeros(data_te.shape[0], dtype=np.float32)),\n",
    "                                      data_params_pred))\n",
    "\n",
    "prd = np.array([item['probabilities'][0] for item in prd])\n",
    "np.savetxt('prd.csv', prd)\n",
    "print('Prediction completed') \n",
    "print('Test prediction time: ', time()-s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = time()\n",
    "sub = towers.predict(input_fn=lambda: input_fn(tf.estimator.ModeKeys.PREDICT,\n",
    "                                      data_sub,\n",
    "                                      pd.Series(np.zeros(data_sub.shape[0], dtype=np.float32)),\n",
    "                                      data_params_pred))\n",
    "\n",
    "sub = np.array([item['probabilities'][0] for item in sub])\n",
    "np.savetxt('sub.csv', sub)\n",
    "print('Submission completed') \n",
    "print('Submission prediction time: ', time()-s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "- Грубым подбором получены следующие гиперпараметры:\n",
    "    - Размер словаря ~38 000 - слова, встречающиеся в обучающей выборке не менее 3 раз\n",
    "    - Размерность векторов слов: 20\n",
    "    - Размер слоя LSTM: 16\n",
    "    - Размеры фильтров сверточного слоя: 2, 3, 4, 5\n",
    "    - Количество фильтров в сверточном слое: 32 (для каждого размера)\n",
    "    - Learning rate: 0.002\n",
    "    - Batch size: 1000\n",
    "- Loss по выборкам:\n",
    "\n",
    "| - | train | test | private |\n",
    "| --- | --- | --- | --- |\n",
    "| log loss | ~$10^{-2}$ | 1.8 | 2.0 |\n",
    "     \n",
    "     Пути снижения степени оверфиттинга:\n",
    "     - точный подбор гиперпараметров\n",
    "     - увеличение датасета \n",
    "     - настройка регуляризации\n",
    "\n",
    "- Один из недостатков модели - векторизация текста по словам:\n",
    "    - требуется более тщательная предобработка текста (выделение корней, лемматизация)\n",
    "    - слова с опечатками приходится выбрасывать из vocabulary <br>\n",
    "    Для решения этих проблем можно реализовать векторизацию n соседних символов (например, триграмм).\n",
    "    \n",
    "- При использовании для слов из обучающей выборки предобученных word2vec с сокращением размерности векторов до 22 качество повысить не удалось. Полная матрица word2vec в рамках этой работы для ускорения расчетов не использовалась. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
